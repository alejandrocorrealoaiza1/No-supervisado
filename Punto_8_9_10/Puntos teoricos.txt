8) Una de las formas para hacer más robusto el PCA es penalizar las soluciones demasiado complejas premiando entonces las más simples a través de la regularización, afectando esos componentes principales que tienen una magnitud grande. También Utilizar validación cruzada puede ayudar a evaluar la robustez de los resultados de PCA. Al repetir PCA en múltiples subconjuntos de los datos y comparar los resultados, se puede identificar cualquier inestabilidad en los resultados de PCA. Por Ultimo, Escalar las características antes de aplicar PCA puede ayudar a hacerlo más robusto

9) La UMAP es un método de aprendizaje no supervisado utilizado para la reducción de la dimensionalidad y la visualización de datos. 
La UMAP utiliza una técnica llamada gráfico de vecinos para construir una topología de los datos. En el gráfico de vecinos, se construye un grafo donde los nodos representan los puntos de datos y las aristas conectan los puntos más cercanos. Esto permite capturar la estructura local de los datos.
Funciones de densidad: UMAP utiliza funciones de densidad para modelar la distribución de los puntos de datos en el espacio de alta dimensión.
la UMAP utiliza la geometría Riemanniana y la optimización de la entropía cruzada para aproximar la topología de los datos en un espacio de baja dimensionalidad mientras se preserva la estructura local de los datos. Esto permite visualizar y explorar datos complejos en una forma interpretable y útil.

10) LDA es una técnica de aprendizaje supervisado y se utiliza para encontrar una proyección de los datos en un espacio de menor dimensión que maximice la separación entre las clases, es decir, que maximice la distancia entre las medias de las clases y minimice la varianza dentro de las clases. 
Principios:
-Análisis de varianza
-Optimización de la función objetivo: maximice la separación entre las clases y minimice la varianza dentro de las clases
-Descomposición de valores propios
-Ortogonalidad: 
LDA utiliza una combinación de análisis de varianza, optimización de la función objetivo, descomposición de valores propios, ortogonalidad y aprendizaje supervisado para encontrar una proyección lineal óptima de los datos de entrada que maximice la separación entre las clases y minimice la varianza dentro de las clases.
